<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
<meta name="description" content="">
<meta name="author" content="">
<link rel="icon" href="/favicon.ico">

<title>EECS 349 Project: Face Alignment Using Texture and Shape Based Model</title>

<!-- Bootstrap core CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/c3.css" rel="stylesheet">

<!-- Custom styles for this template -->
<link href="/css/jumbotron.css" rel="stylesheet">
<link href="/css/jasny-bootstrap.min.css" rel="stylesheet">
<link href="//vjs.zencdn.net/4.12/video-js.css" rel="stylesheet">
<script src="//vjs.zencdn.net/4.12/video.js"></script>
<script src="/js/jquery-1.11.3.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/jquery.form.js"></script>
<script src="/js/jcanvas.min.js"></script>
<script src="/js/jasny-bootstrap.min.js"></script>
<script src="/js/d3.v3.min.js" charset="utf-8"></script>
<script src="/js/c3.js" charset="utf-8"></script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
-->
<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body data-spy="scroll" data-target="#navbar-target" data-offset="50">

<nav id="navbar-target" class="navbar navbar-inverse navbar-fixed-top">
	<div class="container">
		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			<a class="navbar-brand" href="#">EECS 349 Project</a>
		</div>
		<div id="navbar" class="navbar-collapse collapse">
			<ul class="nav navbar-nav navbar-right">
				<li class="active"><a class="tag-scroll" href="#overview">Overview</a></li>
				<li><a class="tag-scroll" href="#introduction">Introduction</a></li>
				<li><a class="tag-scroll" href="#dataset">Dataset</a></li>
				<li><a class="tag-scroll" href="#approach">Approach</a></li>
				<li><a class="tag-scroll" href="#implementation">Implementation</a></li>
				<!--
				<li><a class="tag-scroll" href="#train">Train</a></li>
				<li><a class="tag-scroll" href="#predict">Predict</a></li>
				-->
				<li><a class="tag-scroll" href="#tuning">Tuning</a></li>
				<li><a class="tag-scroll" href="#comparison">Comparison</a></li>
			</ul>
			<!--
	  <form class="navbar-form navbar-right">
	    <div class="form-group">
	      <input type="text" placeholder="Email" class="form-control">
	    </div>
	    <div class="form-group">
	      <input type="password" placeholder="Password" class="form-control">
	    </div>
	    <button type="submit" class="btn btn-success">Sign in</button>
	  </form>
	  -->
		</div><!--/.navbar-collapse -->
	</div>
</nav>

<!-- Main jumbotron for a primary marketing message or call to action -->
<div class="jumbotron">
	<div class="max-con">
		<div class="container">
			<h2 id="overview">Face Alignment Using Texture and Shape Based Model</h2>
			<p>The following video shows what we have done in this project:</p>
			<p align="center">
			<video id="inst-id" class="video-js vjs-default-skin vjs-big-play-centered"
				width="75%" height="360"
				poster="http://video-js.zencoder.com/oceans-clip.png"
				data-setup='{"controls":true,"autoplay":false,"preload":"auto"}'>
				<source src="/video/test.mp4" type='video/mp4' />
			</video>
			</p>
			<h3>Brief</h3>
			<p>This is a project for <a href="http://www.cs.northwestern.edu/~ddowney/courses/349_Spring2015/">EECS 349 Machine Learning</a> instructed by <a href="http://www.cs.northwestern.edu/~ddowney">Doug Downey</a> in Northwestern University. In this project, we implemented an <b>ESR</b> (Explicit Shape Regression) by ourselves according to the original paper, and use <b>AAM</b> (Active Appearance Model) and our own ESR implementation to achieve the face alignment task. We compared the AAM to our implementation of ESR, and also compared the alignment result under different parameters in our implementation. Finally, we also provide a online demo for those interested in our project. </p>

			<h3>Demo</h3>
			<p>
			In our demo for this project, we trained an ESR model with our ESR implementation using the labeled face images from <a href="http://ibug.doc.ic.ac.uk/resources/facial-point-annotations/">IBUG</a> dataset (68 landmarks), and we use the Haar cascade face detector for the face detecting task, which is the first stage of the face keypoints alignment.
			</p>
			<p>
			To try our demo, the only thing you need to do is to upload a picture with one or more faces (not a cartoon image), then the selected picture will be uploaded, the landmarks will finally be marked on the picture.
			</p>

			<canvas id="demo-canvas" width="720" height="480">
				Your browser doesn't support HTML5 canvas
			</canvas>

			<div align="center">
				<form id="demo-upload" enctype="multipart/form-data" action="/upload" method="post">
					<!--<input id="upload-file" type="file" name="image" />-->
					<!--<input type="submit" value="Upload" name="submit" />-->
					<div class="input-margin fileinput fileinput-new input-group" data-provides="fileinput">
						<div class="form-control" data-trigger="fileinput"><i class="fileinput-exists"></i> <span class="fileinput-filename"></span></div>
						<!--<div class="form-control" data-trigger="fileinput"><i class="glyphicon glyphicon-file fileinput-exists"></i> <span class="fileinput-filename"></span></div>-->
						<span class="input-group-addon btn btn-default btn-file"><span class="fileinput-new">Select file</span><span class="fileinput-exists">Change</span><input type="file" name="image" id="upload-file"></span>
						<a href="#" class="input-group-addon btn btn-default fileinput-exists" data-dismiss="fileinput">Remove</a>
					</div>
				</form>
			</div>


			<h3>Code</h3>
			<p>Here we provide our code, you could view our code by clicking the button below.</p>
			<p align="center"><a class="btn btn-primary btn-lg" href="https://github.com/geraint0923/ML-project" role="button">View Repository on Github</a></p>

			<h3>Team</h3>
			<p> 
			<ul>
				<li><a href="http://yangy.me">Yang Yang</a> (yangyang2016 [at] u [dot] northwestern [dot] edu)</li>
				<li>Haomin Hu (haominhu2014 [at] u [dot] northwestern [dot] edu)</li>
				<li>Can Wang (canwang2016 [at] u [dot] northwestern [dot] edu)</li>
				<li>Lijun Tang (hercule2400 [at] gmail [dot] com)</li>
			</ul>
			</p>
		</div>
	</div>
</div>

<div class="container">
	<!-- Example row of columns -->
	<div class="row">
		<h3 id="introduction">Introduction</h3>
		<p>
		Face alignment of facial landmarks can be briefly described as problems of locating and marking key points (landmarks) in images of human faces. Locating facial landmarks such as eyes, nose, mouth and chin, is essential to many applications related to human faces, including face recognition, expression interpretation, and digital face modeling.
		</p>
		<p>
		For exampple, currently the pipeline of face verification consists of four stages: detect, align, represent, classify, where face alignment plays quite a critical rols. An accurate and efficient alignment method could highly improve the accuracy and efficiency of face verification.
		</p>
		<p align="center">
		<img style="margin-bottom:30px;margin-top:30px;" src="/img/intro.jpg">
		</p>
		<p>
		What we are working on is to generate a model from training set with machine learning methods and apply it to locate facial landmarks such as eyes, nose, mouth and chin. However, we are focusing on implementing some well-established models and training strategies, rather than coming up with our own ones. Our work includes:
		<ul>
			<li>Implement Explicit Shape Regression (ESR) algorithm by ourselves train ESR models</li>
			<li>Use an existing Active Appearance Model (AAM) library to train AAM models</li>
			<li>Optimize our ESR implementation by tuning and using better feature selection strategy</li>
			<li>Study how the parameters influence the accuracy of ESR algorithm</li>
			<li>Compare the results of our ESR implementation and AAM</li>
			<li>Build up a online demo on this page to demonstrate ESR model</li>
		</ul>
		</p>
	</div>
	<hr>

	<!-- Example row of columns -->

	<!-- Example row of columns -->
	<div class="row">
		<h3 id="dataset">Dataset</h3>
		We use two dataset for  training for our project:
		<ul>
			<li>
			<a href="http://www.vision.caltech.edu/xpburgos/ICCV13/"><font size="4">COFW (Caltech Occluded Faces in the Wild, 29 landmakrs, 1345 images)</font></a>
			<p>
			COFW dataset is designed to present faces in real-world conditions. Faces show large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food, hands, microphones,â€¨etc.).
			</p>
			<p align="center">
			<img style="margin-bottom:30px;margin-top:30px;" src="/img/dataset/COFW.jpg">
			</p>
			</li>
			<li>
			<a href="http://ibug.doc.ic.ac.uk/resources/facial-point-annotations/"><font size="4">IBUG (Intelligent Behaviour Understanding Group, 68 landmarks, originally 3818 images)</font></a>
			<p>
			Existing facial databases cover large variations including: different subjects, poses, illumination, occlusions etc. However, the provided annotations appear to have several limitations.
			<ul>
				<li>The majority of existing databases provide annotations for a relatively small subset of the overall images.</li>
				<li>The accuracy of provided annotations in some cases is not so good (probably due to human fatigue).</li>
				<li>The annotation model of each database consists of different number of landmarks.</li>
			</ul>
			</p>
			<p>
			These problems make cross-database experiments and comparisons between different methods almost infeasible.  To overcome these difficulties, we propose a semi-automatic annotation methodology for annotating massive face datasets. This is the first attempt to create a tool suitable for annotating massive facial databases.  All the annotations are provided for research purposes ONLY (NO commercial products).
			</p>
			<p align="center">
			<div class="row" style="margin-bottom:50px;margin-top:30px;">
				<div class="col-md-7">
					<img style="margin-left:10px;margin-right:10px;" class="dataset-img" src="/img/dataset/ibug.jpg">
				</div>
				<div class="col-md-5" style="margin-top:30px;">
					<img style="margin-left:10px;margin-right:10px;" class="dataset-img"  src="/img/dataset/ibug_landmark.jpg">
				</div>
			</div>
			</p>
			</li>
		</ul>
		<p>
		Before train the model using the datasets, we should do some preprocessing on these datasets.
		<ul>
			<li><b>Extract the orignal images</b> Since different datasets provide the images in different format (COFW provides the image in Matlabâ€™s .mat forma while IBUG provides individual images in .jpg and .png format), we must extract the images from them so that we donâ€™t need to spend too much time dealing with the input when training the model. We choose to convert them into jpg with an increasing number as the name of images.
			</li>
			<li><b>Generate face rectangles.</b> Since training a ESR model requires normalization for each input face, the face rectangles should be generated before the training process is started. In our project, we use a commonly used face detector -- <a href="http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html">CascadeClassifier</a>, which is provided by OpenCV, to complete this part in preprocess stage. However for AAM, face rectangles are not necessary.
			</li>
			<li><b>Generate(Extract) landmarks coordinates.</b> As mentioned above, COFW provides the groundtruth shape in Matlabâ€™s .mat file while IBUG has .pts files which store the shape information. In our project, we store all the shapes into one single file in plain text.
			</li>
			<li><b>Transform to grayscale images.</b> Because color images have much larger size and the algorithms we use only use gray-scale images as training input. We transform all the images in the datasets into gray-scale images.
			</li>
		</ul>
		</p>
	</div>
	<hr>
	<div class="row">
		<h3 id="approach">Approach</h3>
		<p>
		One way of tackling face alignment problem is to formulate it as an optimization problem. The objective function to minimize is the difference or error between the model given, and the input image. A ideal model could be presented by the following equation:
		$$M(I) = \arg \min\limits_{M} \sum\limits_{i=1}^{|D|} || M(I_i)- \hat{S_i}||_2$$
		where \(M(I)\) is the generated model by minimizing the error between the predicted landmarks coordinates and the pre-labeled coordinates (groundtruth shape, which is from the dataset). \(D\) is the dataset for training, while \(\hat{S_i}\) is the groundtruth shape for the \(i\)-th image in the training set.
		</p>
		<p>
		Also, we need a method to evaluate the alignment result, we use the following method to evaluate the error, the lower the error is, the better the result appear:
		$$error(M) = \sum\limits_{i=1}^{|D|} \frac{\sqrt{||M(I_i)- \hat{S_i}||_2}}{|\hat{S_i}|}$$
		</p>
		<p>
		In this project, we use two different models to complete the face alignment:
		</p>
		<ul>
			<li>
			<b>Active Appearance Model (AAM)</b>

			<p>
			A typical optimization based algorithm we are to study and implement in this project is Active Appearance Model (AAM), one well-established local texture and shape based model for face alignment.
			</p>

			<p>
			The training of AAM can be break into 2 steps: 1) generating shapes, 2) generating appearance. Both models are formulated as linear combinations of bases, as shown in formula below: 

			$$A = A_0 + \sum\limits_{i=1}^{m} \lambda_i A_i$$
			$$s = s_0 + \sum\limits_{i=1}^{n} p_i s_i$$
			</p>

			<p>
			The linear bases are computed by applying PCA to normalized training images. The coefficients are parameters to be decided in prediction phase. 
			</p>

			<p>
			The prediction of AAM is basically fitting the model into the input image by adjusting the parameters, the coefficients in training phase. This could be formulated as an optimization problem in which our goal is to minimize the squared difference(error) between the real input image and the models trained as shown in the following equation: 
			$$\sum\limits_{x\in s_0}[A_0(x) + \sum\limits_{i=1}^{m} \lambda_i A_i(x) - I(W(x; p))]^2$$
			where \(I\) is the input image, \(W(x; p)\) is the mapping function from trained model's relative coordinates to real image's absolute coordiantes.
			</p>
			<p>
			There are a handful of well-established methods to solve this non-linear optimation problem. For instance the gradient descent algorithm which moves in the direction of negative gradient of current point. An improved optimization approach we employed, the inverse compositional algorithm, approxiamates the gradient with second derivative in taylor expansion. Detailed mathematical tricks is available in the paper.
			</p>

			</li>
			<li>
			<b>Explicit Shape Regression (ESR)</b>
			<p>
			Another algorithm we studied and implemented is Explicit Shape Regression(ESR). 
			</p>

			<p>
			As opposed to AAM which generates the model with batch of training instances, ESR does it in a different way, additively fetching training images one at a time. Starting from the initial face shape \(S^0\), it trains the shape iteratively as shown below:
			$$S^t = S^{t-1} + R^t(I, S^{t-1}), t = 1,...,T$$
			where \(S^i\) is the shape model in iteration #i, \(R^i\) is the regressor calculated as follows:
			$$R^t = \arg \min \sum\limits_{i=1}^{N} || \hat S_i - (S_i^{t-1} + R(I, S_i^{t-1}))||$$

			</p>
			<p>
			In practise fern is introduced when determining the shape. A fern is a composition of F (5 in our implementation) features and thresholds that divide the feature space (and all training samples) into \(2^F\) bins. Each bin is acting independently and is associated with a regressor output:
			$$\delta S_b = \arg \min \sum\limits_{i=1}^{N} || \hat S_i - (S_i + \delta S))|| = \sum \frac {(\hat S_i - S_i)} {2^F} $$
			where \(S_i\) is the estimated output in previous step.
			</p>

			<p>
			To avoid overfitting, a shrinkage is performed: 
			$$\delta S_b = \frac {1} {(1 + \frac {\beta} {2^F})} \sum \frac{(\hat S_i - S_i)} {2^F} $$
			</p>

			<p>
			The prediction is based on the fern strategy mentioned earlier: when the model is given an image, it calculates certain attributes and throws the image into corresponding bin and predicts based on the bin regressor output.
			</p>

			<p>
			Here we provide a set of intermedia results which demonstrate how the error decreases and how the shape regression works when iterating.
			</p>
			<div class="row">
				<div class="col-md-8">
					<div id="train-chart" class="chart-div"></div>
				</div>
				<div class="col-md-4">
					<img id="train-pic" src="/img/draw/00.jpg" />
				</div>   
			</div>
			<p>
			<b>NOTE:</b> You could put your mouse over the chart and move to see the face alignment status on different iteration.
			</p>

			</li>
		</ul>

	</div>
	<hr>
	<div class="row">
		<h3 id="implementation">Implementation</h3>
		<p>When we implemented ESR algorithm, we found there are serveral algorithm details which are not specified in the original paper:</p>
		<ul>
			<li><b>Select the pixels used to calculate the features.</b>
			<p>
			There are two different methods to select the pixels:
			<ul>
				<li>In the original paper, it says that the pixel is random picked in each regressor. But in the process of implementing our own ESR, we find out that randomly picking is not a good idea since the randomly picked pixel might be too far away from the landmark and also be not informative enough to provided enough information when used as feature calculation.</li>
				<li>We come up with another idea to select the pixels and use it in our implementation. Our idea is to randomly pick the pixels around the known landmarks. The landmarks are called landmarks because they are special, and there are more information we could extract from the pixels around the landmarks. And we select the pixels averagely from all the landmarks so that the selected pixels could cover all the landmarks.</li>
			</ul>
			After the test, we find that our idea works much better than the naive one. And our implementation finally use this method.
			</p>
			</li>
			<li><b>Determine the threshold when selecting bin.</b>
			<p>
			There are also two different ways to dtermine the threshold for each bin:
			<ul>
				<li>One is using the mean value of all the image. </li>
				<li>The other one is to use the median value as the threshold. </li>
			</ul>
			We test both of these two methods to see which one is better. And we finally find out that the second method provides a lower error, and use this method in our implementation. We guess that the latter works better just because there might be some outliers in the training data, so the mean value might have a large bias while median value could better split the training set without being affected by the outliers.
			</p>
			</li>
		</ul>
		<p>
		<b>NOTE:</b> In our ESR implementation, we store our output model as JSON format, which makes the model more human-readable and easy to debug.
		</p>
	</div>
	<hr>
	<!-- Example row of columns -->
	<!--
	<div class="row">
		<h3 id="train">Train</h3>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec id elit non mi porta gravida at eget metus. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus. Etiam porta sem malesuada magna mollis euismod. Donec sed odio dui. </p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec id elit non mi porta gravida at eget metus. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus. Etiam porta sem malesuada magna mollis euismod. Donec sed odio dui. </p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec sed odio dui. Cras justo odio, dapibus ac facilisis in, egestas eget quam. Vestibulum id ligula porta felis euismod semper. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus.</p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div> 
	</div>
	<hr>
	-->

	<!-- Example row of columns -->
	<!--
	<div class="row">
		<h3 id="predict">Predict</h3>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec id elit non mi porta gravida at eget metus. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus. Etiam porta sem malesuada magna mollis euismod. Donec sed odio dui. </p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec id elit non mi porta gravida at eget metus. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus. Etiam porta sem malesuada magna mollis euismod. Donec sed odio dui. </p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec sed odio dui. Cras justo odio, dapibus ac facilisis in, egestas eget quam. Vestibulum id ligula porta felis euismod semper. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus.</p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div> 
	</div>
	<hr>
	-->

	<!-- Example row of columns -->
	<div class="row">
		<h3 id="tuning">Tuning</h3>
		<p>
		To optimize the performance of ESR algorithm, we must test different values on the different parameters used in ESR. In ESR algorithm, there are four parameters which could be tuned according the paper, we test these parameters in our ESR implementation.
		</p>
		<p>
		The following tuning results are performed on a post-clean test set including about 500 images with 68 landmarks.
		</p>
		<ul>
			<li><b>Number of the used weak regressors in the first level (typically 10 according to the original paper)</b>
			<div class="row">
				<div class="col-md-8">
					<div id="regressor-chart" class="chart-div"></div>
				</div>

				<div class="col-md-4" style="margin-top:20px">
					<p>This parameter will change the number of the week regressors in the first level in our ESR implmentation.</p>
					<p>
					From the left chart, we could find out that the error decreases as this parameter increase, even though the error increases a little when the number of regressors is set to 12, we guess that it should be the noise since our ESR implmentation will randomly choose the initial shape when predicting a input image. We can conclude that increasing the number of regressor will slightly improve the alignment result.
					</p>
				</div>   
			</div>
			</li>
			<li><b>Number of the trained ferns in the second level (typically 500)</b>
			<div class="row">
				<div class="col-md-4" style="margin-top:20px">
					<p>This parameter will change the number of the small ferns in the second level in our ESR implmentation.</p>
					<p>
					The right charts shows that the error decreases as the number of ferns at the beginning, however it rises up a little after the number of ferns is larger than 500. We tries generate this curve several times but the result looks similar. It might indicate the overfitting on the training set since the candidate pixels is the same in each regressor, too many ferns results in the using the pixels that is not that informative.
					</p>
				</div>   
				<div class="col-md-8">
					<div id="fern-chart" class="chart-div"></div>
				</div>
			</div>
			</li>
			<li><b>Number of the random sampled initial shape for each image in the training dataset (typically 20)</b>
			<div class="row">
				<div class="col-md-8">
					<div id="augment-chart" class="chart-div"></div>
				</div>

				<div class="col-md-4" style="margin-top:20px">
					<p>This parameter will change the number of the initial shapes (in the dataset augment stage) for each image in augment stage in training set in our ESR implmentation.</p>
					<p>
					The chart tells us that the error will always decrease as the number of initial shapes increases, even the error increases a little when the parameter reaches 70. And we could also find out that the error decreases very quickly, so this parameter should be a very important factor in ESR model. However, increasing this parameter will also increase the training time and predicting time.
					</p>
				</div>   
			</div>
			</li>
			<li><b>Number of the pixelx pairs which are selected as features in each ferns (typically 5)</b>
			<div class="row">
				<div class="col-md-4" style="margin-top:20px">
					<p>This parameter will change the number of features (actually pairs of pixel) used to split the training set in each fern in our ESR implmentation.</p>
					<p>
					From the right chart, we could learn that the error will decrease as the the number of features increases at the beginning, which makes the alignment result better. But the error begins to rise up after the number of features is larger than 6, which indicates the overfitting on the training set. As we mention when we tune the number of ferns, selecting a feature which is not that informative will not help, and makes the model overfitting, which leads to the decreasing of accuracy.
					</p>
				</div>   
				<div class="col-md-8">
					<div id="feature-chart" class="chart-div"></div>
				</div>
			</div>
			</li>
		</ul>

	</div>
	<!-- Example row of columns -->
	<div class="row">
		<h3 id="comparison">Comparison</h3>

		<p>
		The learning curve is shown in the figure below:
		</p>
		<div id="curve-chart" class="chart-div"></div>
		<p>
		From the curve we can see that AAM did better at first but EST got the last laugh. As AAM is a texture-based model and employs PCA to generate the shapes, it is not that sensitive to size of training set comparing to ESR. ESR, however, has to train each bin with adequate number of instances to achieve good performance and therefore relies heavily on the size of training set. 
		</p>
		<p align="center">
		<img style="width:66%;margin-bottom:30px;margin-top:30px;" src="/img/cmp.jpg">
		</p>
		<p>
		The above image shows the comparison between the results of our ESR implementation and AAM. The left is the result of our ESR implementation while the right is the reesult of AAM. Both models are trained from IBUG dataset with 870 images after data cleaning. We could find out that the result of our ESR implementation is much better than the result of AAM.
		</p>
		<!-- Example row of columns -->
		<!--
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec id elit non mi porta gravida at eget metus. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus. Etiam porta sem malesuada magna mollis euismod. Donec sed odio dui. </p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec id elit non mi porta gravida at eget metus. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus. Etiam porta sem malesuada magna mollis euismod. Donec sed odio dui. </p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div>
		<div class="col-md-4">
			<h2>Heading</h2>
			<p>Donec sed odio dui. Cras justo odio, dapibus ac facilisis in, egestas eget quam. Vestibulum id ligula porta felis euismod semper. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus.</p>
			<p><a class="btn btn-default" href="#" role="button">View details &raquo;</a></p>
		</div>
		-->
	</div>

	<hr>

	<footer>
		<p>&copy; EECS 349 Project: Face Alignment Using Texture and Shape Based Model</p>
		<p>Team Member: <a href="http://yangy.me">Yang Yang</a>, Haomin Hu, Can Wang, Lijun Tang</p>
	</footer>
</div> <!-- /container -->


<!-- Bootstrap core JavaScript
    ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->

<script>
/*
   <form id="demo-upload" enctype="multipart/form-data" action="/upload" method="post" onSubmit="submitUpload()">

   function processForm(e) {
   if(e.preventDefault) {
   e.preventDefault();
   }
// TODO: submit the upload form
return false;
}
var form = document.getElementById('demo-upload');
if (form.attachEvent) {
form.attachEvent("submit", processForm);
} else {
form.addEventListener("submit", processForm);
}
 */
var curUUID = "/img/downey.jpg";
var img_width, img_height, canvas_width, canvas_height, img_scale;
var canvas = undefined;
var img_list = [];
var train_pic = document.getElementById("train-pic");
function pad_zero(num, size) {
	var s = "0" + num;
	return s.substr(s.length-size);
}
for (var i = 0; i <= 10; i++) {
	var p = new Image();
	p.src = "/img/draw/"+pad_zero(i, 2)+".jpg";
	img_list.push(p);
}

function canvasDrawPoint(x, y) {
	var sx = (canvas_width - img_width) / 2,
	    sy = (canvas_height - img_height) / 2;
	canvas.drawEllipse({
fillStyle: '#0f0',
x: sx + x, y: sy + y,
width: 3, height: 3,
});
}

function canvasDrawFace(face) {
	//console.log("face draw called");
	for(var i = 0; i < face.length; i++) {
		var x = face[i].x * img_width, y = face[i].y * img_height;
		canvasDrawPoint(x, y);
	}
}

function requestAlignment(uuid) {
	$.ajax({
type: "POST",
url: "/do_alignment",
data: {"uuid": uuid},
success: function(data) {
//alert("success!!"+data);
//console.log(data.uuid+" => "+curUUID);
if(data.uuid != curUUID)
return;
if(data.faces.length == 0) {
alert("no face is detected");
}
for(var i = 0; i < data.faces.length; i++) {
canvasDrawFace(data.faces[i]);
}
},
error: function(err) {
alert("error!!");
},
//dataType: "application/json"
});
}
function canvasDrawImage(img) {
	img_height = img.height;
	img_width = img.width;
	var hscale = canvas_height/img_height, wscale = canvas_width/img_width;
	if(hscale > wscale) {
		if(wscale < 1) {
			img_scale = wscale;
			img_width = canvas_width;
			img_height = img_height * img_scale;
		} else {
			img_scale = 1;
		}
	} else {
		if(hscale < 1) {
			img_scale = hscale;
			img_height = canvas_height;
			img_width = img_width * img_scale;
		} else {
			img_scale = 1;
		}
	}
	canvas.clearCanvas();
	canvas.drawImage({
source: img,
x: canvas_width/2, y: canvas_height/2,
scale: img_scale,
fromCenter: true
});
//console.log("draw image done");
}
function drawChart(chart_id, chart_url, cur_key, cur_value) {
	/*
	   d3.tsv(chart_url, function(error, data) {
	   console.log(data);
	   });
	 */
	c3.generate({
bindto: "#" + chart_id,
data: {
url: chart_url,
x: cur_key,
}
});
console.log(chart_id);
}

$(document).ready(function() {
		//initialize the charts
		drawChart("regressor-chart", "/data/regressor_test.csv", "regressor", "error");
		drawChart("fern-chart",	     "/data/fern_test.csv", "fern", "error");
		drawChart("augment-chart",   "/data/augment_test.csv", "augment", "error");
		drawChart("feature-chart",     "/data/feature_test.csv", "feature", "error");
		c3.generate({
			bindto: "#train-chart",
			data: {
				url: "/data/err_decrease.csv",
				onmouseover: function(d) {
					train_pic.src = img_list[d.index].src;
				},
				x: "iter",
			}
		});
		c3.generate({
			bindto: "#curve-chart",
			data: {
				url: "/data/learning_curve.csv",
				x: "rate",
			},
			/*
			colors: {
				ESR: '#00ff00',
				AAM: '#0000ff',
			}
			*/
		});

		canvas = $('#demo-canvas');
		canvas_height = canvas.height();
		canvas_width = canvas.width();
		var init_img = new Image();
		init_img.src = curUUID;
		init_img.onload = function() {
		canvasDrawImage(init_img);
		requestAlignment(curUUID);
		};
		$(".tag-scroll").click(function(e) {
			e.preventDefault();
			var offset = 50; //42;
			var dest = this.href;
			$('html,body').animate({scrollTop:$($(this).attr("href")).offset().top-offset}, 800);
			});
		$("#upload-file").change(function(e) {
				//alert($("#upload-file").val());
				var file_path = e.target.files[0];
				//alert(file_path);
				if(file_path == undefined || file_path.length < 1)
				return;
				var url = URL.createObjectURL(file_path);
				var img = new Image();
				img.src = url;
				img.onload = function() {
				$("#demo-upload").submit();
				canvasDrawImage(img);
				};
				});
		$('#demo-upload').ajaxForm(function(data) {
				curUUID = JSON.parse(data).uuid;
				requestAlignment(curUUID);
				//alert(curUUID);
				});
		$('#demo-canvas').drawPolygon({
strokeStyle: 'black',
strokeWidth: 4,
x: 200, y: 100,
radius: 50,
sides: 3
});
});
</script>
</body>
</html>
